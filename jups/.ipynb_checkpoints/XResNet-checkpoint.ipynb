{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import datasets\n",
    "path = datasets.untar_data(datasets.URLs.IMAGENETTE_160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoneReduce():\n",
    "    def __init__(self, loss_func):\n",
    "        self.loss_func = loss_func\n",
    "        self.old_red = None\n",
    "        \n",
    "    def __enter__(self):\n",
    "        if hasattr(self.loss_func, \"reduction\"):\n",
    "            self.old_red = getattr(self.loss_func, \"reduction\")\n",
    "            setattr(self.loss_func, \"reduction\", \"none\")\n",
    "            return self.loss_func\n",
    "        else:\n",
    "            return partial(self.loss_func, reduction=\"none\")\n",
    "        \n",
    "    def __exit__(self):\n",
    "        if self.old_red is not None:\n",
    "            setattr(self.loss_func, \"reduction\", self.old_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-9f7f031f3324>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-9f7f031f3324>\"\u001b[1;36m, line \u001b[1;32m20\u001b[0m\n\u001b[1;33m    loss = lin_comb(loss1, loss2, self.lambda)\u001b[0m\n\u001b[1;37m                                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class MixUp(Callback):\n",
    "    ORDER = 90\n",
    "    def __init__(self, a = 0.4):\n",
    "        self.distrib = Beta(tensor([a]), tensor([a]))\n",
    "        \n",
    "    def begin_fit(self):\n",
    "        self.old_loss_func = self.trainer.loss_func\n",
    "        self.trainer.loss_func = self.loss_func\n",
    "        \n",
    "    def after_fit(self):\n",
    "        self.trainer.loss_func = self.old_loss_func\n",
    "        \n",
    "    def loss_func(self, pred, yb):\n",
    "        if not self.in_train:\n",
    "            return self.old_loss_func(pred, yb)\n",
    "        # a loss is either a rank 1 tensor or mean/sum\n",
    "        # here we turned off reduction, so e.g. loss1 is a tensor (rank1)\n",
    "        with NoneReduce(self.old_loss_func) as loss_func:\n",
    "            loss1 = loss_func(pred, yb)\n",
    "            loss2 = loss_func(pred, self.yb1)\n",
    "        loss = lin_comb(loss1, loss2, self.lambda)\n",
    "    return reduce_loss(loss, getattr(self.old_loss_func, \"reduction\", \"mean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, eps, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.red = reduction\n",
    "        \n",
    "    def forward(self, out, targ):\n",
    "        c = out.size()[-1]\n",
    "        log_preds = F.log_softmax(out, dim=-1)\n",
    "        loss = reduce_loss(-log_preds.sum(dim=-1), self.red)\n",
    "        nll = F.nll_loss(log_preds, targ, reduction=self.red)\n",
    "        return loss/c * eps + (1-eps)*nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_cnn(module):\n",
    "    if getattr(module, \"bias\", None) is not None:\n",
    "        nn.init.constant_(module.bias, 0)\n",
    "    if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.kaiming_normal_(module.weight)\n",
    "    for m in module.children():\n",
    "        init_cnn(m)\n",
    "        \n",
    "def conv(ni, nf, ks=3, stride=1, bias=False):\n",
    "    return nn.Conv2d(ni, nf, kernel_size=ks, \n",
    "                     stride=stride, padding=ks//2, bias=bias)\n",
    "        \n",
    "def conv_layer(ni, nf, ks=3, stride=1, zero_bn=False, act=True):\n",
    "    bn = nn.BatchNorm2d(nf)\n",
    "    nn.init.constant_(bn.weight, 0. if zero_bn else 1.)\n",
    "    layers = [conv(ni, nf, ks, stride), bn]\n",
    "    if act:\n",
    "        layers.append(nn.ReLU())\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noop(x): return x\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, expansion, ni, nh, stride=1):\n",
    "        super().__init__()\n",
    "        nf, ni = nh * expansion, ni * expansion\n",
    "        layers = [conv_layer(ni, nh, 1)]\n",
    "        \n",
    "        if expansion == 1:\n",
    "            layers += [conv_layer(nh, nf, 3, stride=stride, \n",
    "                                  zero_bn=True, act=False)]\n",
    "        else:\n",
    "            layers += [\n",
    "                conv_layer(nh, nh, 3, stride=stride),\n",
    "                conv_layer(nh, nf, 1, zero_bn = True, act=False)\n",
    "            ]\n",
    "            \n",
    "        self.A = nn.Sequential(*layers)\n",
    "        self.B = noop if ni == nf else conv_layer(ni, nf, 1, act=False)\n",
    "        self.pool = noop if stride == 1 else nn.AvgPool2d(2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return nn.ReLU(inplace=True)(self.A(x) + self.B(self.pool(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XResNet(nn.Sequential):\n",
    "    @classmethod\n",
    "    def create(cls, expansion, layers, c_in=3, c_out=1000):\n",
    "        nfs = [c_in, (c_in+1)*8, 64, 64]\n",
    "        stem = [conv_layer(nfs[i], nfs[i+1], stride=2 if i == 0 else 1)\n",
    "               for i in range(3)]\n",
    "        \n",
    "        nfs = [64//expansion, 64, 128, 256, 512]\n",
    "        res_layers =[cls._make_layer(expansion, nfs[i], nfs[i+1],\n",
    "                                    n_blocks=1, stride=1 if i ==0 else 2)\n",
    "                    for i, l in enumerate(layers)]\n",
    "        res = cls(*stem,\n",
    "                 nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "                 *res_layers,\n",
    "                 nn.AdaptiveAvgPool2d(1), Flatten(),\n",
    "                 nn.Linear(nfs[-1]*expansion, c_out))\n",
    "        init_cnn(res)\n",
    "        return res\n",
    "        \n",
    "    @staticmethod\n",
    "    def _make_layer(expansion, ni, nf, n_blocks, stride):\n",
    "        return nn.Sequential(\n",
    "        *[ResBlock(expansion, ni if i == 0 else nf, nf, stride if i == 0 else 1)\n",
    "         for i in range(n_blocks)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xresnet18(**kwargs): return XResNet.create(1, [2, 2, 2, 2], **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
